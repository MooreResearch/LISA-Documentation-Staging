<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="/usr/share/javascript/mathjax/tex-mml-chtml.js" type="text/javascript"></script>
</head>
<body>
<p><strong>General Relativity and Gravitational Waves</strong></p>
<p><strong>Session 2: Tensor Equations</strong></p>
<h2 id="SessOverv">Overview of this Session</h2>
<p>Here is a summary of what I plan to do in the sections in this session:</p>
<ul>
<li><p><strong>Tensors</strong> introduces the concept of a tensor.</p></li>
<li><p><strong>Covariant Equations</strong> shows how to use tensors to construct an equation that is valid in any inertial reference frame.</p></li>
<li><p><strong>Maxwell’s Equations</strong> illustrates the power of tensor equations by “deriving” Maxwell’s equations from the requirements that Gauss’s law and the definition of the electric field be frame-independent.</p></li>
</ul>
<h2 id="tensors">Tensors</h2>
<p>Last time we introduced the concept of a four-vector <span class="math inline">\(\mathbf{A}\)</span> whose four components transform as follows: <span class="math display">\[\begin{aligned}
    A&#39;^\alpha = \Lambda^{\alpha}_{\beta} A^\beta.\end{aligned}\]</span> (with a sum over the four values of the index <span class="math inline">\(\beta\)</span> implied by the Einstein summation convention. Consider now a frame-independent (<strong>scalar</strong>) field <span class="math inline">\(\Phi(t,x,y,z)\)</span> (describing something perhaps like temperature as a function of position and time). The <strong>four-gradient</strong> <span class="math inline">\(\partial_\alpha\Phi \equiv \partial\Phi/\partial x^\alpha\)</span> has four components, but how do these components transform as we change reference frames? Consider the time component of this object in the primed reference frame. According to the rules of multivariable calculus, we have <span class="math display">\[\frac{\partial\Phi}{\partial t^\prime} =
\frac{\partial t}{\partial t^\prime} \frac{\partial\Phi}{\partial t}
+ \frac{\partial x}{\partial t^\prime} \frac{\partial\Phi}{\partial x}
+ \frac{\partial y}{\partial t^\prime} \frac{\partial\Phi}{\partial y}
+ \frac{\partial z}{\partial t^\prime} \frac{\partial\Phi}{\partial z}
= (\Lambda^{-1})^{t}_{t}\frac{\partial\Phi}{\partial t}
+ (\Lambda^{-1})^{x}_{t}\frac{\partial\Phi}{\partial x}
+ (\Lambda^{-1})^{y}_{t}\frac{\partial\Phi}{\partial y}
+ (\Lambda^{-1})^{z}_{t}\frac{\partial\Phi}{\partial z}\]</span> since evaluating <span class="math inline">\(\partial t/\partial t^\prime, \partial t/\partial x^\prime,
\partial t/\partial y^\prime,\)</span> and <span class="math inline">\(\partial t/\partial z^\prime\)</span> involves taking partial derivatives of the <em>inverse</em> Lorentz transformation equations and those partials yield simply the constant coefficients of that linear transformation. Similar expressions apply to the other components, so we can compactly write <span class="math display">\[\partial^{\:\prime}_{\alpha}\Phi = (\Lambda^{-1})^{\beta}_{\alpha}\:(\partial_{\beta}\,\Phi)\]</span></p>
<p>So the gradient of a frame-independent field does not transform like a four-vector, but has a closely related and similarly simple transformation law. We call any set of four components that transform according to an <em>inverse</em> Lorentz transformation a <strong>covector</strong>.</p>
<p>How does the four-gradient <span class="math inline">\(\partial_\alpha A^\beta\)</span> of a four-vector field <span class="math inline">\(\mathbf{A}\)</span> transform? The transformation rules imply that <span class="math display">\[\partial^{\:\prime}_{\alpha} A^{\prime\,\beta} =
(\Lambda^{-1})^{\mu}_{\alpha}\frac{\partial}{\partial x^{\mu}}
\left(\Lambda^{\beta}_{\nu}A^{\nu}\right)
= (\Lambda^{-1})^{\mu}_{\alpha}\Lambda^{\beta}_{\nu}
\left(\partial_{\mu} A^{\nu}\right)\]</span></p>
<p>because the coefficients <span class="math inline">\(\Lambda^{\beta}_{\nu}\)</span> of the Lorentz transformation do not themselves depend on position. This 16-component quantity therefore also has a simple transformation law, which involves a Lorentz transformation factor for a superscript index and an inverse Lorentz transformation for the subscript index. We call such a quantity a <strong>second-rank tensor</strong>. By analogy, we define an <strong><em>n</em>th-rank tensor</strong> <span class="math inline">\(T^{\alpha\cdots}_{\beta\cdots}{}^{\gamma\cdots}\)</span> to be an <em>n</em>-index object (with <span class="math inline">\(4^n\)</span> components) that transforms according to <span class="math display">\[T&#39;^{\alpha\cdots}_{\beta\cdots}{}^{\gamma\cdots} =
\Lambda^{\alpha}_{\mu}\cdots(\Lambda^{-1})^{\nu}_{\beta}\cdots
\Lambda^{\gamma}_{\sigma}\cdots T^{\mu\cdots}_{\nu\cdots}{}^{\sigma\cdots}\]</span></p>
<p>that is, a Lorentz-transformation factor for every upper (superscript) index and an inverse-Lorentz-transformation factor for every lower (subscript) index. The vertical position of tensor indices therefore carries very important information about how the tensor transforms. The horizontal position of the indices on some tensors can also be physically significant, an issue which we will consider in more depth later.</p>
<p>The “tensor” concept generalizes and extends the four-vector/covector concept. Indeed, a four-vector is a rank-one tensor with one upper index. A covector is a rank-one tensor with one lower index. A scalar (frame-independent quantity) is a zero-rank tensor.</p>
<p>The gradient of a four-vector is one example of an operation that combines tensors to yield another tensor. Another example is what we can call the <strong>tensor product</strong> <span class="math inline">\(A^\mu B^\nu\)</span> of two four-vectors <span class="math inline">\(A^\mu\)</span> and <span class="math inline">\(B^\nu\)</span>. This is a 16-component object whose <span class="math inline">\(\mu\text{-}\nu\)</span> component is the product of the <span class="math inline">\(\mu\)</span>th component of <span class="math inline">\(\mathbf{A}\)</span> and the <span class="math inline">\(\nu\)</span>th component of <span class="math inline">\(\mathbf{B}\)</span>. It transforms as follows: <span class="math display">\[T&#39;^{\mu\nu} \equiv A&#39;^{\mu}B&#39;^{\nu} =
(\Lambda^{\mu}_{\alpha}A^{\alpha})(\Lambda^{\nu}_{\beta}B^{\beta}) =
\Lambda^{\mu}_{\alpha}\Lambda^{\nu}_{\beta}(A^{\alpha} B^{\beta}) =
\Lambda^{\mu}_{\alpha}\Lambda^{\nu}_{\beta}T^{\alpha\beta}\]</span></p>
<p>So the tensor product of four-vectors is indeed a second-rank tensor with two upper indices.</p>
<p>Another tensor operation is summing over an upper and lower index (a process we call <strong>contraction</strong> over those indices). Consider a second-rank tensor object <span class="math inline">\(T^{\alpha}_{\beta}\)</span> with one upper and one lower index, and suppose that we sum over the upper and lower index (if we imagine the components of <span class="math inline">\(\mathbf{T}\)</span> arranged in a matrix, this would be the same as summing the matrix’s diagonal elements). This operation produces a one-component object that transforms as <span class="math display">\[T&#39;^{\alpha}_{\alpha} =
\Lambda^{\alpha}_{\mu}(\Lambda^{-1})^{\nu}_{\alpha}T^{\mu}_{\nu}
= \delta^{\nu}_{\mu}T^{\mu}_{\nu}\]</span> because the matrix product of the inverse Lorentz transformation and the Lorentz transformation is the identity matrix, which in index notation is the Kronecker delta (summing the Lorentz transformation coefficients over the <span class="math inline">\(\alpha\)</span> index is equivalent to doing a matrix product in the order <span class="math inline">\([\Lambda^{-1}][\Lambda]\)</span>. But if we now sum over, say, the <span class="math inline">\(\mu\)</span> index, only the terms with <span class="math inline">\(\nu = \mu\)</span> are nonzero, meaning that the expression above reduces to <span class="math display">\[T&#39;^{\alpha}_{\alpha} = T^{\nu}_{\nu}\]</span></p>
<p>So the value of the contracted tensor is <em>frame-independent</em>: it is indeed a zeroth-rank tensor (scalar). Generally, summing over an upper and lower index of a <em>n</em>th-rank tensor produces a new tensor with rank <span class="math inline">\(n-2\)</span>. This is why the Einstein summation convention is specific about summing over one upper and one lower index: as you can easily show, the sums <span class="math display">\[\sum T_{\mu}^{\mu} \quad \text{and} \quad \sum T^{\mu}_{\mu}\]</span></p>
<p>produce single numbers, but these numbers are not frame-independent scalars (they are not tensors).</p>
<p>Now, as its name indicates, the metric tensor <span class="math inline">\(\eta_{\mu \nu}\)</span> is a second-rank tensor with two lower indices. Here is how we can prove it. (The process will also nicely illustrate some issues about using index notation that will be useful to us as we go along.) The invariance of the spacetime separation implies that <span class="math display">\[\eta_{\mu \nu}\Delta x&#39;^{\mu}\Delta x&#39;^{\nu} =
\eta_{\alpha \beta}\Delta x^{\alpha} \Delta x^{\beta} =
\eta_{\alpha \beta}(\Lambda^{-1})^{\alpha}_{\gamma}\:
\Delta x&#39;^{\gamma}
(\Lambda^{-1})^{\beta}_{\sigma}\:\Delta x&#39;^{\sigma}
= [(\Lambda^{-1})^{\alpha}_{\gamma}(\Lambda^{-1})^{\beta}_{\sigma}
\eta_{\alpha \beta}]\Delta x&#39;^{\gamma}\Delta x&#39;^{\sigma}\]</span></p>
<p>Note that we can freely rearrange the order of items because in each term of the implied sums, the items are simply numerical values, and multiplication is commutative. Also, because addition is associative, it does not matter in which order we perform the implied sums (over 16 terms on the left side, 256 terms on the right side). This flexibility is what makes index notation <em>much</em> easier than matrix notation for dealing with equations like this. Another flexibility is that the name we give to a summed index is completely arbitrary. It is good practice (as I have done initially) to keep index names distinct. But one can take advantage of the arbitrary nature of index names to rename summed indices in convenient ways, as long as we don’t give indices that describe distinct sums the same index names (as then it becomes ambiguous about what exactly we are summing over). In this particular case, we can rename the sum over the <span class="math inline">\(\gamma\)</span> index on the right side so that it becomes a sum over a <span class="math inline">\(\mu\)</span> index, and similarly rename <span class="math inline">\(\sigma \rightarrow \nu\)</span>: <span class="math display">\[\eta_{\mu \nu}\Delta x&#39;^{\mu}\Delta x&#39;^{\nu}
= [(\Lambda^{-1})^{\alpha}_{\mu}(\Lambda^{-1})^{\beta}_{\nu}
\eta_{\alpha \beta}]\Delta x&#39;^{\mu}\Delta x&#39;^{\nu}
\quad \text{or} \quad
0 = [\eta_{\mu \nu}-(\Lambda^{-1})^{\alpha}_{\mu}
(\Lambda^{-1})^{\beta}_{\nu}
\eta_{\alpha \beta}]\Delta x&#39;^{\mu}\Delta x&#39;^{\nu}\]</span></p>
<p>In this case, renaming allows us to subtract the left side from the right side and pull out the common factor of <span class="math inline">\(\Delta x^{\prime\,\mu}\Delta x^{\prime\,\nu}\)</span> from both terms. We cannot simply now divide through by <span class="math inline">\(\Delta x^{\prime\,\mu}\Delta x^{\prime\,\nu}\)</span> because we are summing over <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> and a sum can be zero even though individual terms in the sum are not. But in this case, we know that the sum must be zero no matter what the values of the coordinate differences <span class="math inline">\(\Delta x^{\prime\,\mu}\)</span> and <span class="math inline">\(\Delta x^{\prime\,\nu}\)</span> might actually have. Indeed, we can judiciously choose pairs of events to isolate terms in the sums to prove that in fact the quantity in square brackets must be zero for every possible choice of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span>. For example, suppose that I choose a pair of events that have coordinate separation in the primed frame that is purely in the <span class="math inline">\(t^\prime\)</span>-direction: <span class="math inline">\(\Delta x^\prime
= \Delta y^\prime = \Delta z^\prime = 0\)</span>. Then all terms in the sum above except the term with <span class="math inline">\(\mu = \nu = t\)</span> are zero, and we see that the <em>t-t</em> component of the term in the bracket must be zero. We can similarly constrain all the other components. So because the original equation must work for all possible event coordinate-separations, we must have <span class="math display">\[0 = \eta_{\mu \nu}-(\Lambda^{-1})^{\alpha}_{\mu}
(\Lambda^{-1})^{\beta}_{\nu}
\eta_{\alpha \beta} \quad \Rightarrow \quad
\eta_{\mu \nu} =
(\Lambda^{-1})^{\alpha}_{\mu}
(\Lambda^{-1})^{\beta}_{\nu}
\eta_{\alpha \beta}\]</span> The metric tensor <span class="math inline">\(\eta_{\mu\nu}\)</span> by definition has the same components in every inertial reference frame, and the equation above shows that this is consistent with its being a second-rank tensor with two lower indices.</p>
<p>In a similar way, one can show that the Kronecker delta <span class="math inline">\(\delta^{\mu}_{\nu}\)</span> transforms like a second-rank tensor with one upper and one lower index, and that the matrix inverse <span class="math inline">\(\eta^{\mu\nu}\)</span> of the metric tensor, defined such that <span class="math display">\[\eta^{\mu\alpha}\eta_{\alpha\nu} =
\delta^{\mu}_{\nu}\]</span> (and which happens in special relativity to have the same components as <span class="math inline">\(\eta_{\mu\nu}\)</span>) transforms like a second-rank tensor with two upper indices. Finally, since the tensor product and contraction operations produce tensors, we see that the operations <span class="math display">\[A_{\mu} = \eta_{\mu \nu} A^{\nu} \quad \text{and} \quad
B^{\mu} = \eta^{\mu \nu} B_{\nu}\]</span></p>
<p>produce a covector representation of the four-vector <span class="math inline">\(\mathbf{A}\)</span> and a four-vector representation of the covector <span class="math inline">\(\mathbf{B}\)</span>.</p>
<p>We can also add tensors that have the same rank and index position. For example, the set of four components <span class="math inline">\(C^\mu \equiv A^\mu + B^\mu\)</span> transforms like <span class="math display">\[C&#39;^{\mu} = A&#39;^{\mu} + B&#39;^{\mu}
= \Lambda^{\mu}_{\nu} A^{\nu} + \Lambda^{\mu}_{\alpha} B^{\alpha}
= \Lambda^{\mu}_{\nu} (A^{\nu} + B^{\nu}) = \Lambda^{\mu}_{\nu} C^{\nu}\]</span> In the next-to-last step, I renamed the summed <span class="math inline">\(\alpha\)</span> index to <span class="math inline">\(\nu)\)</span> so that I could pull out the common Lorentz transformation coefficient. We see that this equation implies that the four components <span class="math inline">\(C^\mu\)</span> really do transform like the components of a first-rank tensor <span class="math inline">\(\mathbf{C}\)</span>. You can see that a similar proof will apply to other tensor sums as long as the number and positions of the indices are the same.</p>
<p>So, to summarize, we have a well-defined set of operations on tensors that produce tensors:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Tensor Addition:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(p_\text{tot}^\mu = p_1^\mu + p_2^\mu\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Tensor Product:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(A^\mu B^\nu = T^{\,\mu\nu}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Contraction:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(\delta^{\mu}_{\mu}=4\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Lowering indices:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(A_\mu = \eta_{\mu\nu}A^\nu\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Raising indices:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(B^\mu = \eta^{\mu\nu}B_\nu\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Renaming summed indices:</strong></td>
<td style="text-align: left;">example: <span class="math inline">\(\delta^{\mu}_{\mu} = \delta^{\nu}_{\nu} = 4\)</span></td>
</tr>
</tbody>
</table>
<p>The importance of all of this is that if we create a tensor equation (for example <span class="math inline">\(A^\mu = B^\mu\)</span> or any of the equations above), we can be assured that if it is true in any one inertial reference frame it is true in <em>every</em> inertial reference frame. This is because if we change reference frames, the component(s) on the right side of the equation transform in exactly the same way as the components on the left side. (Of course, the tensor on the right side of the equation must have the same number of indices and in the same positions for this to work: equating a second-rank tensor to a scalar, for example, would make no sense.) This means that we can write absolute physics equations that work in <em>every</em> inertial reference frame. For example, the tensor equation <span class="math inline">\(d(\eta_{\mu\nu}p^\mu p^\nu)/d\tau = d(p_\mu p^\nu)/d\tau = 0\)</span>, which says that the magnitude of a particle’s four-momentum (its mass) does not change in time, works in every inertial frame, no matter what the components of the four-momentum might be in that particular frame. This allows us to compactly and generally state physical laws that are <em>automatically</em> consistent with the principle of relativity. This is extremely powerful, as we will see shortly.</p>
<p>But before we get to that, I want to point out that if you are new to index notation, one can easily write equations that superficially look good but are nonsense, or perform operations that turn perfectly good equations into nonsense.</p>
<p>In a moment, I will give you some rules that will help you avoid making mistakes. But first of all, let me define some terms. A <strong>summed index</strong> in an equation is an index that we are summing over, while a <strong>free index</strong> can take on any of its four possible values that we choose. For example in the equation <span class="math inline">\(A_\mu = \eta_{\mu\nu}A^\nu\)</span>, the <span class="math inline">\(\mu\)</span> index is free while the <span class="math inline">\(\nu\)</span> index is summed. The fact that we can arbitrarily choose the value of the <span class="math inline">\(\mu\)</span> index means that this compact tensor equation stands for four component equations: <span class="math display">\[\begin{aligned}
A_t = \eta_{t\nu}A^\nu &amp;= \eta_{tt}A^t + \eta_{tx}A^x
+ \eta_{tx}A^y + \eta_{tz}A^z \\
A_x = \eta_{x\nu}A^\nu &amp;= \eta_{xt}A^t + \eta_{xx}A^x
+ \eta_{xy}A^y + \eta_{xz}A^z \\
A_y = \eta_{y\nu}A^\nu &amp;= \eta_{yt}A^t + \eta_{yx}A^x
+ \eta_{yy}A^y + \eta_{yz}A^z \\
A_z = \eta_{z\nu}A^\nu &amp;= \eta_{zt}A^t + \eta_{zx}A^x
+ \eta_{zy}A^y + \eta_{zz}A^z
\end{aligned}\]</span> Secondly, in an equation in which there is an explicit sum, for example <span class="math display">\[\frac{d}{d\tau}(\eta_{\mu\nu}p^{\,\mu} p^\nu) =
\eta_{\mu\nu}\frac{dp^{\,\mu}}{d\tau}p^\nu + \eta_{\mu\nu}p^{\,\mu}\frac{dp^\nu}{d\tau}\]</span> (which expresses the product rule of calculus in a case where we are evaluating the time derivative of the squared magnitude of a particle’s four-momentum), we call the two items in the right-most expression <strong>terms</strong>, and the three quantities that are multiplied together in each of those two terms <strong>factors</strong>.</p>
<p>Now we are ready to state the rules.</p>
<ol>
<li><p><strong>Free indices.</strong> We cannot add tensor or equate tensor quantities that do not have the same number of indices: it makes no sense to equate or add quantities that have different numbers of components. Similarly, it makes no sense if the free indices are not in the same vertical positions, because then the quantities will not transform alike. Therefore the free indices on the right side of an equation must be have the same number and vertical positions as those on the left, and the same applies to any added terms. Moreover, all free indices should have the same names as their counterparts in other terms or on other sides of the equation. Examples of bad equations are: <span class="math display">\[\text{\textbf{Bad: }} A^2 = \eta_{\mu\nu}A^{\alpha\beta}, \quad
A^\mu = B^\nu, \quad A_\mu = B^\mu\]</span> The <strong>only exception</strong>: by convention, setting a tensor equal to zero is allowed. For example <span class="math inline">\(p_{\text{tot}}^\mu = 0\)</span> means that all the components of the total momentum of a system are zero. This is because any tensor whose components are all zero in some frame (no matter how many indices it has and no matter what the positions of those indices are) will transform to all zeros in any other coordinate system. So there is no point in attaching indices to such a zero-valued tensor.</p></li>
<li><p><strong>Renaming free indices.</strong> One can legally rename any <em>free</em> index with a different Greek letter (the choice of letter names is arbitrary) as long as (1) you avoid names already in use by free or summed indices, and (2) you rename <em>every</em> occurrence of that index. For example: <span class="math display">\[\text{\textbf{Bad: }} A&#39;^{\mu} = \Lambda^{\mu}_{\nu}A^{\nu}
\quad \rightarrow \quad A&#39;^{\alpha} = \Lambda^{\mu}_{\nu}A^{\nu}
\quad \quad
\text{\textbf{Good: }} A&#39;^{\mu} = \Lambda^{\mu}_{\nu}A^{\nu}
\quad \rightarrow \quad A&#39;^{\alpha} = \Lambda^{\alpha}_{\nu}A^{\nu}\]</span></p></li>
<li><p><strong>Renaming summed indices.</strong> One can legally rename any <em>summed</em> index in a term as long as (1) you rename both occurrences of the index and (2) you avoid names <em>already appearing in the same term</em>. This avoids ambiguities. For example renaming the <span class="math inline">\(\nu\)</span> index in the equation below to <span class="math inline">\(\mu\)</span> <span class="math display">\[\text{\textbf{Bad: }} A&#39;^{\mu} = \Lambda^{\mu}_{\nu}A^{\nu}
\quad \rightarrow \quad A&#39;^{\mu} = \Lambda^{\mu}_{\mu}A^{\mu}\]</span> confuses what the sums really are. The first equation clearly has four implicit terms on the left, but the second equation is ambiguous: is the <span class="math inline">\(\mu\)</span> index free or summed? Are we doing a sum or not?</p>
<p>On the other hand, renaming summed indices to agree with the same name in <em>different</em> terms is not only allowed, but can be very useful. For example renaming the summed index <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\nu\)</span> in the last term in the middle equality below <span class="math display">\[\text{\textbf{Good: }}
C&#39;^{\mu} \equiv A&#39;^{\mu} + B&#39;^{\mu}
= \Lambda^{\mu}_{\nu}A^{\nu} + \Lambda^{\mu}_{\alpha}B^{\alpha}
= \Lambda^{\mu}_{\nu}(A^{\nu} + B^{\nu})
\equiv \Lambda^{\mu}_{\nu}C^{\nu}\]</span> not only is legal but allows us to group common terms together and simplify the equation.</p></li>
<li><p><strong>When in doubt, write it out.</strong> If you are ever uncertain about what is legal and what is not, write out the implicit sums (if practical). You all have lots of practice with complicated equations that don’t involve implicit sums. It may even help to just insert the implied summation symbols.</p></li>
</ol>
<h3 id="exercise-good-or-bad">Exercise: Good or Bad?</h3>
<p>Consider the equations listed below. Answer A = Violates Rule 1, B = Violates Rule 2, C = Violates Rule 3, D = OK for each equation. For each acceptable equation, specify how many equations it implicitly represents (A = 1, B = 4, C = 16, D = 64, E = 256).</p>
<ol>
<li><p><span class="math inline">\(\eta_{\mu\nu}u^{\mu} u^{\nu} = -1\)</span></p></li>
<li><p><span class="math inline">\(p^{\alpha} = m u^{\alpha}\delta^{\mu}_{\beta}\)</span></p></li>
<li><p><span class="math inline">\(\Lambda^{\alpha}_{\beta}A^{\beta} = A&#39;^{\alpha}
\quad \text{renamed to} \quad
\Lambda^{\alpha}_{\beta}A^{\beta} = A&#39;^{\mu}\)</span></p></li>
<li><p><span class="math inline">\(\eta_{\mu\nu}(\Lambda^{-1})^{\nu}_{\alpha} =
\eta_{\mu\beta}\Lambda^{\beta}_{\alpha}\)</span></p></li>
<li><p><span class="math inline">\(\eta_{\mu\nu}A^{\mu} B^{\nu} = 0 \quad \text{renamed to} \quad
\eta_{\mu\mu}A^{\mu} B^{\mu} = 0\)</span></p></li>
<li><p><span class="math inline">\(\dfrac{dp^{\alpha}}{d\tau} = q F^{\mu\nu}u^{\alpha}\)</span></p></li>
<li><p><span class="math inline">\(T_{\mu\nu\alpha} + T_{\alpha\mu\nu} + T_{\nu\alpha\mu} = 0\)</span></p></li>
<li><p><span class="math inline">\(0 = \eta_{\mu\nu}A^{\mu} B^{\nu} + \eta_{\alpha\beta}A^{\alpha} C^{\beta}
\quad \text{renamed to} \quad
0 = \eta_{\mu\nu}A^{\mu} B^{\nu} + \eta_{\mu\nu}A^{\mu} C^{\nu}\)</span></p></li>
</ol>
<h2 id="maxwells-equations">Maxwell’s Equations</h2>
<p>If we can write a law of physics as a tensor equation, it will have exactly the same form in all inertial reference frames. Such a <strong>manifestly covariant</strong> equation automatically satisfies the principle of relativity. The tensor formalism therefore provides a powerful tool for finding relativistic generalizations of pre-relativistic laws of physics. In this section, I will illustrate the process by “deriving” Maxwell’s equations by seeking tensor expressions of Gauss’s law and the definition of the electric field for a particle at rest.</p>
<p>This section assumes that you already know Maxwell’s equations and for static electric fields, we can define an electromagnetic potential <span class="math inline">\(\phi\)</span> such that <span class="math inline">\(\vec{E} = -\vec{\nabla}\phi\)</span>.</p>
<p>Start with the Newtonian equation for the force on a particle with charge <span class="math inline">\(q\)</span> at rest: <span class="math display">\[\frac{d\vec{p}}{dt} = q\vec{E} = -q\vec{\nabla}\phi\]</span> where <span class="math inline">\(\phi\)</span> is the electrostatic potential, and the Gauss’s law expressed in terms of the potential: <span class="math display">\[\label{Poisson}
    \vec{\nabla} \cdot \vec{E} = -\nabla^2\phi
    = \frac{\rho}{\varepsilon_0}\]</span> where <span class="math inline">\(\vec{\nabla}^2 = \vec{\nabla} \cdot \vec{\nabla}
= \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}\)</span>. Let’s assume that we know experimentally that these laws are true in static situations.</p>
<p>The first step to finding a tensor generalization of these laws is to determine the transformation properties of <span class="math inline">\(\rho\)</span>: is this a scalar, a component of a four-vector, a component of a second-rank tensor or what? Charge itself must be a relativistic <em>scalar</em>: if the charge of a particle were not frame-independent, then the charges an atom’s electrons (which orbit the nucleus at all kinds of different speeds) would not exactly cancel the charge of the protons at rest in the atom’s nucleus, meaning that different atoms would have different nonzero net charges, something we do not observe. So let’s assume that charge is a relativistic scalar.</p>
<p>What does this mean about the charge <em>density</em>? Consider a small box of volume <span class="math inline">\(V\)</span> filled with a total charge <span class="math inline">\(q\)</span> that is at rest the unprimed inertial frame. Suppose we look at the box in the primed frame, where the box is moving with <span class="math inline">\(x\)</span>-velocity <span class="math inline">\(-\beta\)</span>. In this frame, the box has the same total charge <span class="math inline">\(q\)</span> (because charge is a relativistic scalar), but it will be observed to have a smaller volume <span class="math inline">\(V^\prime = V\sqrt{1-\beta^2}\)</span> because the box’s length in the <span class="math inline">\(x\)</span> direction is observed to be Lorentz-contracted by a factor of <span class="math inline">\(\sqrt{1-\beta^2}\)</span>. Therefore, the charge <em>density</em> in the primed frame is <span class="math display">\[\rho^\prime = \frac{q}{V^\prime} = \frac{q}{V\sqrt{1-\beta^2}} = \gamma \rho\]</span> Moreover, in the primed frame, the charge is moving with a velocity <span class="math inline">\(\vec{v}^{\,\prime} = -\vec{\beta}\)</span> in the -<span class="math inline">\(x\)</span> direction, so it has a nonzero current density whose <span class="math inline">\(x\)</span> component should be <span class="math inline">\(-\rho^\prime\beta\)</span>.</p>
<p>Now, suppose we define a <strong>four-current</strong> <span class="math inline">\(\mathbf{J}\)</span> so that its components in any inertial reference frame are <span class="math inline">\(J^t \equiv \rho,
J^x = \rho v_x, J^y = \rho v_y, J^z = \rho v_z\)</span>. According to the Lorentz transformation equations, if we have <span class="math inline">\(J^t = \rho, J^x = J^y =
J^z = 0\)</span>, then in the primed frame we should have <span class="math display">\[\begin{aligned}
\rho^\prime = J^{\prime\,t} &amp;= \gamma J^t - \gamma \beta J^x
= \gamma \rho - 0 = \gamma \rho\\
J^{\prime\,x} &amp;= -\gamma \beta J^t + \gamma J^x = -\gamma \beta \rho + 0
= -\rho^\prime \beta\\
J^{\prime\,y} &amp;= J^y = 0 \\
J^{\prime\,z} &amp;= J^z = 0
\end{aligned}\]</span> consistent with our earlier results. So we see that the charge density <span class="math inline">\(\rho\)</span> transforms as the time component of a four-vector.</p>
<p>But this means that the right side of the relativistic generalization of <span class="math inline">\(-\vec{\nabla}^2\phi = \rho/\varepsilon_0\)</span> must be the four-vector <span class="math inline">\(\mathbf{J}/\varepsilon_0\)</span>. What about the left side? A plausible tensor generalization of is <span class="math inline">\(-\partial_\mu \partial^\mu \equiv -\eta^{\mu\nu}\partial_\mu\partial_\nu=\)</span> : for a static potential field, the added time-derivative will be zero, so the two expressions are equivalent. But <span class="math inline">\(-\partial_\mu \partial^\mu\)</span> transforms as a relativistic scalar, so if the left side is to transform as the time component of a four-vector, then <span class="math inline">\(\phi\)</span> must be the time component of a four-vector. Let’s call the components of that four-vector <span class="math inline">\(A^\alpha\)</span>. Therefore, the natural relativistic generalization of the Poisson equation is <span class="math display">\[-\partial_\mu \partial^\mu A^\alpha = \frac{1}{\varepsilon_0}J^\alpha\]</span> However, this is not the most general equation, because in a static situation any time-derivatives of the four-potential will be zero. Therefore the more general equation <span class="math display">\[\label{PoisGen}
-\partial_\mu (\partial^\mu A^\alpha + b\,\partial^\alpha A^\mu)
= \frac{1}{\varepsilon_0}J^\alpha\]</span> where <span class="math inline">\(b\)</span> is any frame-independent constant, is also possible, because the time component of the above in a static situation becomes <span class="math display">\[-\partial_\mu (\partial^\mu A^t + b\,\partial^{\,t} A^\mu)
= \frac{1}{\varepsilon_0}J^t  \quad \Rightarrow \quad
-\partial_\mu (\partial^\mu \phi + b\cdot 0) = \frac{1}{\varepsilon_0}\rho
\quad \Rightarrow \quad
-\nabla^2\phi = \frac{1}{\varepsilon_0}\rho\]</span> consistent with equation <a href="#Poisson" data-reference-type="ref" data-reference="Poisson">[Poisson]</a>. So we will take equation <a href="#PoisGen" data-reference-type="ref" data-reference="PoisGen">[PoisGen]</a> to be its relativistic generalization.</p>
<p>Now let’s look at the force equation. Since we know that the electrostatic potential is the time component of a four-vector, the generalization of the force equation must look something like <span class="math display">\[\frac{dp^\alpha}{d\tau} = -q\,\partial^\alpha A^\mu\]</span> But this can’t be right, because the free indexes don’t match. We need an additional covector on the right to contract with the <span class="math inline">\(A^\mu\)</span> in such a way that for a particle at rest, only the time component <span class="math inline">\(A^t = \phi\)</span> survives. The natural choice is the covector version of the particle’s four-velocity <span class="math inline">\(u_\mu = \eta_{\mu\nu}u^\nu\)</span>, because for a particle at rest, <span class="math inline">\(u^t = 1, u^x = u^y = u^z = 0 \quad \Rightarrow \quad
u_t = \eta_{t\nu}u^\nu = \eta_{tt}u^t + 0 = -1, u_x = \eta_{x\nu}u^\nu
= \eta_{xx}u^x = 0\)</span>, and similarly <span class="math inline">\(u_y = u_z = 0\)</span>. So a more credible generalization of the equation above would be <span class="math display">\[\frac{dp^\alpha}{d\tau} = +q\,\partial^\alpha A^\mu u_\mu\]</span> However, this is again not the most general form, because <span class="math display">\[\label{Fwh}
\frac{dp^\alpha}{d\tau} = q(\partial^\alpha A^\mu
+ h\,\partial^{\,\mu} A^\alpha) u_\mu\]</span> (where <span class="math inline">\(h\)</span> is another scalar constant) because for a particle at rest, the new term involves only a time-derivative of <span class="math inline">\(A^\alpha\)</span>, which would be zero in a static situation.</p>
<p>However, in this case, we can constrain the value of <span class="math inline">\(h\)</span>. The time-derivative of the squared magnitude of the particle’s four-momentum (that is, the square of its rest-energy <span class="math inline">\(m\)</span>) must be zero no matter what is happening with the electromagnetic potentials. Therefore we must have <span class="math display">\[0 = \frac{d}{d\tau}(-m^2) = \frac{d}{d\tau}(p^\alpha \eta_{\alpha\beta}p^\beta) =
\frac{dp^\alpha}{d\tau}\eta_{\alpha\beta}p^\beta
+ p^\alpha\eta_{\alpha\beta}\frac{dp^\beta}{d\tau} =
\frac{dp^\alpha}{d\tau}\eta_{\alpha\beta}p^\beta
+ p^\beta\eta_{\beta\alpha}\frac{dp^\alpha}{d\tau} =
2\frac{dp^\alpha}{d\tau}\eta_{\alpha\beta}p^\beta\]</span> where in the next-to-last step, I renamed the summed index <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\beta\)</span> and the summed index <span class="math inline">\(\beta\)</span> to <span class="math inline">\(\alpha\)</span> in the second term I used the fact that the metric tensor is symmetric, so that <span class="math inline">\(\eta_{\alpha\beta} = \eta_{\beta\alpha}\)</span>. If we note that <span class="math inline">\(\eta_{\alpha\beta}p^\beta = p_\alpha = mu_\alpha\)</span>, and substitute in equation <a href="#Fwh" data-reference-type="ref" data-reference="Fwh">[Fwh]</a>, we see that <span class="math display">\[0 = 2qm(\partial^\alpha A^\mu
+ h\,\partial^{\,\mu} A^\alpha) u_\alpha  u_\mu\]</span> Now, at first this looks impossible, because the particle’s four-velocity could be anything and the field derivatives could be anything, so how could we be sure that this is zero? But note that if we choose <span class="math inline">\(h = -1\)</span>, <span class="math display">\[(\partial^\alpha A^\mu -\,\partial^{\,\mu} A^\alpha) u_\alpha  u_\mu = 
\partial^\alpha A^\mu u_\alpha u_\mu -
\partial^{\,\mu} A^\alpha u_\alpha  u_\mu = 
\partial^\alpha A^\mu u_\alpha u_\mu -
\partial^{\,\alpha} A^\mu u_\mu  u_\alpha = 0\]</span> where in the next-to-last step, I renamed the summed indices <span class="math inline">\(\mu \leftrightarrow \alpha\)</span> in the second term, and in the last step I recognized that the order in which we multiply <span class="math inline">\(u_\alpha u_\mu\)</span> is irrelevant. So we see that we can ensure that the fields have no effect on a particle’s rest mass by choosing <span class="math inline">\(h = -1\)</span> in equation <a href="#Fwh" data-reference-type="ref" data-reference="Fwh">[Fwh]</a>, yielding <span class="math display">\[\frac{dp^\alpha}{d\tau} = q(\partial^\alpha A^\mu
- \,\partial^{\,\mu} A^\alpha) u_\mu\]</span></p>
<p>We can also constrain <span class="math inline">\(b\)</span> in equation <a href="#PoisGen" data-reference-type="ref" data-reference="PoisGen">[PoisGen]</a>. Suppose we multiply both sides by <span class="math inline">\(\partial_\alpha\)</span> and sum over <span class="math inline">\(\alpha\)</span>: <span class="math display">\[\frac{1}{\varepsilon_0}\partial_\alpha J^\alpha
= -\partial_\alpha\partial_\mu (\partial^\mu A^\alpha + b\,\partial^\alpha A^\mu)
= -\partial_\alpha\partial_\mu \partial^\mu A^\alpha
- b\,\partial_\mu\partial_\alpha \partial^\mu A^\alpha
= -(1+b)\partial_\alpha\partial_\mu \partial^\mu A^\alpha\]</span> where in the next-to-last step I have renamed the summed indices <span class="math inline">\(\mu \leftrightarrow \alpha\)</span> in the second term, and in the last step I noted that the order in which we take the partial derivatives does not matter. But charge conservation requires that <span class="math inline">\(\partial_\alpha J^\alpha = 0 \Rightarrow \frac{\partial \rho}{\partial t} = -\vec{\nabla} \cdot \vec{J}\)</span> (which basically says that when charge is conserved, the rate of change of the charge in a tiny box must be equal to negative the outflow of charge through the box’s faces). The only way to ensure that charge is conserved no matter how the fields vary is to insist that <span class="math inline">\(b = -1\)</span>.</p>
<p>So our final proposed tensor equations for electrodynamics are <span class="math display">\[\frac{dp^\alpha}{d\tau} = q(\partial^\alpha A^\mu
- \,\partial^{\,\mu} A^\alpha) u_\mu
\quad \text{and} \quad
\partial_\mu (\partial^\alpha A^\mu - \partial^\mu A^\alpha)
= \frac{1}{\varepsilon_0}J^\alpha\]</span> where I have flipped the terms in the second equation to get rid of the overall minus sign. Note that quantity in parentheses is a second-rank antisymmetric tensor that appears in both equations. We can give its six independent components (arranged as a square matrix below) letter names <span class="math inline">\(E_x, E_y, E_z, B_x, B_y, B_z\)</span> as follows: <span class="math display">\[\begin{aligned}
F^{\mu\nu} &amp;=
\begin{pmatrix}
\partial^tA^t - \partial^tA^t &amp; \partial^tA^x - \partial^xA^t &amp; \partial^tA^y - \partial^yA^t &amp; \partial^tA^z - \partial^zA^t \\
\partial^xA^t - \partial^tA^x &amp; \partial^xA^x - \partial^xA^x &amp; \partial^xA^y - \partial^yA^x &amp; \partial^xA^z - \partial^zA^x \\
\partial^yA^t - \partial^tA^y &amp; \partial^yA^x - \partial^xA^y &amp; \partial^yA^y - \partial^yA^y &amp; \partial^yA^z - \partial^zA^y \\
\partial^zA^t - \partial^tA^z &amp; \partial^zA^x - \partial^xA^z &amp; \partial^zA^y - \partial^yA^z &amp; \partial^zA^z - \partial^zA^z
\end{pmatrix}\\
&amp;\equiv
\begin{pmatrix}
0&amp;E_x &amp; E_y &amp; E_z \\
-E_x &amp; 0 &amp; B_z &amp; -B_y \\
-E_y &amp; -B_z &amp; 0 &amp; B_x \\
-E_z &amp; B_y &amp; -B_x &amp; 0
\end{pmatrix}
\end{aligned}\]</span></p>
<p>Note that <span class="math inline">\(\partial^\alpha \equiv \eta^{\alpha\beta}\partial_\beta\)</span> implies that <span class="math inline">\(\partial^t = \eta^{t\beta}\partial^\beta = \eta^{tt}\partial_t
= -\partial_t, \partial^x = \eta^{x\beta}\partial^\beta = \eta^{xx}\partial_t
= +\partial_x,\)</span> and similarly that <span class="math inline">\(\partial^y = \partial_y\)</span> and <span class="math inline">\(\partial^z = \partial_z\)</span> (because <span class="math inline">\(\eta^{\mu\nu}\)</span> has the same components as <span class="math inline">\(\eta_{\mu\nu}\)</span>). This means that the definitions stated above imply that <span class="math display">\[\vec{E} = -\vec{\nabla}\phi - \frac{\partial\vec{A}}{\partial t},
    \quad \quad \vec{B} = \vec{\nabla} \times \vec{A}\]</span> (We see that consistency with relativity implies that an electric field must be accompanied by a new field <span class="math inline">\(\vec{B}\)</span> that we can calculate in terms of the spatial components of the four-potential <span class="math inline">\(\mathbf{A}\)</span>!).</p>
<p>In terms of the field tensor <span class="math inline">\(\mathbf{F}\)</span>, our electromagnetic field equations become <span class="math display">\[\frac{dp^\alpha}{d\tau} = qF^{\alpha\mu} u_\mu
    \quad \text{and} \quad
    \partial_\mu F^{\alpha\mu}
    = \frac{1}{\varepsilon_0}J^\alpha\]</span> We can easily see that the first of these equations is the Lorentz force law: for example, its <span class="math inline">\(x\)</span> component is <span class="math display">\[\begin{aligned}[b]
\frac{dp^x}{d\tau} &amp;= q(F^{xt}u_t + F^{xx}u_x = F^{xy}u_y + F^{xz}u_z)
= q\frac{1}{\sqrt{1-v^2}}(-E_x(-1) + 0 + B_z v_y - B_y v_z)
\\ &amp;\Rightarrow \quad
\frac{dp^x}{dt}
= q(-E_x(-1) + 0 + B_z v_y - B_y v_z) = qE_x + q(\vec{v}\times\vec{B})_x
\end{aligned}\]</span> because <span class="math inline">\(d\tau = dt\sqrt{1-v^2}\)</span> and this allows us to cancel out the <span class="math inline">\(\sqrt{1-v^2}\)</span> factors from both sides. The time component of <span class="math inline">\(\partial_\mu F^{\alpha\mu} = J^\alpha/\varepsilon_0\)</span> is Gauss’s law: <span class="math display">\[\partial_t F^{tt} + \partial_x F^{tx} + \partial_y F^{ty} + \partial_z F^{tz}
= 0 + \frac{\partial E_x}{\partial x}+ \frac{\partial E_y}{\partial y}
+ \frac{\partial E_y}{\partial y}
= \frac{J^t}{\varepsilon_0} = \frac{\rho}{\varepsilon_0}\]</span> The other components of <span class="math inline">\(\partial_\mu F^{\alpha\mu} = 
J^\alpha/\varepsilon_0\)</span> are the three components of the Ampere-Maxwell law.</p>
<p>What of the other Maxwell equations? It turns out that the definition of the field tensor in terms of potentials implies the identity <span class="math display">\[\label{FId}
\partial^\alpha F^{\mu\nu} + \partial^\nu F^{\alpha\mu}
+ \partial^\mu F^{\nu\alpha} = 0\]</span> Most of the 64 component equations implied by this tensor relation are trivially zero, but the components equations that are not yield Gauss’s law for the magnetic field and components of Faraday’s law.</p>
<p>The point is that we have “derived” Maxwell’s equations simply by finding generalizations of the Newtonian equation for electrostatic that are (1) covariant tensor equations (expressing consistency with the principle of relativity) (2) consistent with the idea that charge is a relativistic scalar, (3) consistent with charge conservation and (4) consistent with the requirement that electromagnetic fields don’t mess with a charged particle’s mass. I say “derived” in quotes, because we have not shown that our solution is unique, only that it works. Still, this is an amazing illustration of both the idea that Maxwell’s equations are relativistically necessary consequences of electrostatics and more generally the power of covariant tensor equations to expose consequences of the principle of relativity.</p>
<h3 id="exercise-the-ampere-maxwell-law.">Exercise: The Ampere-Maxwell Law.</h3>
<p>Show that <span class="math inline">\(\partial_\mu F^{\alpha\mu} = J^\alpha/\varepsilon_0\)</span> with <span class="math inline">\(\alpha = x\)</span> yields the <span class="math inline">\(x\)</span>-component of the Ampere-Maxwell law, which (in the unit system we are using) is <span class="math inline">\(\vec{\nabla}\times\vec{B}
-\partial\vec{E}/\partial t = \vec{J}/\varepsilon_0\)</span>.</p>
<h3 id="exercise-checking-equation-fid.">Exercise: Checking Equation <a href="#FId" data-reference-type="ref" data-reference="FId">[FId]</a>.</h3>
<p>Show that equation <a href="#FId" data-reference-type="ref" data-reference="FId">[FId]</a> follows from the definition of the field tensor components.</p>
<h3 id="exercise-gausss-law-for-the-magnetic-field.">Exercise: Gauss’s law for the Magnetic Field.</h3>
<p>Find one choice of values for the indices <span class="math inline">\(\alpha, \mu,\)</span> and <span class="math inline">\(\nu\)</span> in equation <a href="#FId" data-reference-type="ref" data-reference="FId">[FId]</a> that yields Gauss’s law for the magnetic field. Are there other choices that yield the same? How many copies of this equation do you think we have in equation <a href="#FId" data-reference-type="ref" data-reference="FId">[FId]</a>?</p>
<h3 id="exercise-the-time-component-of-the-force-law.">Exercise: The Time Component of the Force Law.</h3>
<p>What does the time component of <span class="math inline">\(dp^\alpha/d\tau = qF^{\alpha\beta}u_\beta\)</span> tell us?</p>
<h2 id="homework-problems">Homework Problems</h2>
<ol>
<li><p>Prove that the Kronecker delta <span class="math inline">\(\delta^{\mu}_{\nu}\)</span>, which is defined in <em>all</em> inertial frames to be 1 if <span class="math inline">\(\mu = \nu\)</span> and zero otherwise, correctly obeys the tensor transformation law for a tensor with one upper and one lower index.</p></li>
<li><p>Consider a second-rank tensor <span class="math inline">\(\mathbf{T}\)</span> that is symmetric in some inertial reference frame: <span class="math inline">\(T_{\mu\nu}
= T_{\nu\mu}\)</span>. Prove that it is symmetric in <em>all</em> inertial reference frames. Show that the property of antisymmetry <span class="math inline">\(F^{\mu\nu} = - F^{\nu\mu}\)</span> is similarly frame-independent.</p></li>
<li><p>Find a combination of values for the indices <span class="math inline">\(\alpha, \mu,\)</span> and <span class="math inline">\(\nu\)</span> that yield a component of Faraday’s law, which in our unit system is <span class="math inline">\(\vec{\nabla}\times\vec{E} + \partial{\vec{B}}/\partial t = 0.\)</span></p></li>
</ol>
</body>
</html>
